{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b19fff-8f42-4e9f-a73e-00cff106805a",
   "metadata": {},
   "source": [
    "# M-Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34723a72-1601-4685-a0ba-bff544425d48",
   "metadata": {
    "id": "34723a72-1601-4685-a0ba-bff544425d48"
   },
   "source": [
    "In this notebook, we'll explore small prompt engineering techniques and recommendations that will help us elicit responses from the models that are better suited to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba193cc-d8a0-4ad2-8177-380204426859",
   "metadata": {
    "id": "fba193cc-d8a0-4ad2-8177-380204426859"
   },
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv('../../open_ai.env')) # read the env file from parent directory\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cfc93-21e0-498f-9650-37bc6ddd514d",
   "metadata": {
    "id": "502cfc93-21e0-498f-9650-37bc6ddd514d"
   },
   "source": [
    "# Formatting the answer with Few Shot Samples.\n",
    "\n",
    "To obtain the model's response in a specific format, we have various options, but one of the most convenient is to use Few-Shot Samples. This involves presenting the model with pairs of user queries and example responses.\n",
    "\n",
    "Large models like GPT-3.5 respond well to the examples provided, adapting their response to the specified format.\n",
    "\n",
    "Depending on the number of examples given, this technique can be referred to as:\n",
    "* Zero-Shot.\n",
    "* One-Shot.\n",
    "* Few-Shots.\n",
    "\n",
    "With One Shot should be enough, and it is recommended to use a maximum of six shots. It's important to remember that this information is passed in each query and occupies space in the input prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8344712-06d7-4c24-83d8-f36d62926e5e",
   "metadata": {
    "id": "a8344712-06d7-4c24-83d8-f36d62926e5e"
   },
   "outputs": [],
   "source": [
    "# Function to call the model.\n",
    "def return_OAIResponse(user_message, context):\n",
    "    client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "    newcontext = context.copy()\n",
    "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=newcontext,\n",
    "            temperature=1,\n",
    "        )\n",
    "\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611d73d-9330-466d-b705-543667e1b561",
   "metadata": {
    "id": "f611d73d-9330-466d-b705-543667e1b561"
   },
   "source": [
    "In this zero-shots prompt we obtain a correct response, but without formatting, as the model incorporates the information he wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
    "outputId": "4c4a9f4f-67c9-4a11-837f-1a1fd6b516ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The winner of the 2010 Formula 1 World Championship was Sebastian Vettel. He secured his first World Drivers' Championship driving for Red Bull Racing.\n"
     ]
    }
   ],
   "source": [
    "#zero-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in F1.'}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2010?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8",
   "metadata": {
    "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8"
   },
   "source": [
    "For a model as large and good as GPT 3.5, a single shot is enough to learn the output format we expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
    "outputId": "5278df23-8797-4dc2-9340-ac29c1318a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The winner of the 2011 Formula 1 World Championship was Sebastian Vettel. He drove for the Red Bull Racing team.\n"
     ]
    }
   ],
   "source": [
    "#one-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2000 f1 championship?\n",
    "     Driver: Michael Schumacher.\n",
    "     Team: Ferrari.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2011?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c454a8-181b-482b-873a-81d6ffde4674",
   "metadata": {
    "id": "32c454a8-181b-482b-873a-81d6ffde4674"
   },
   "source": [
    "Smaller models, or more complicated formats, may require more than one shot. Here a sample with two shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
    "outputId": "a6f90f5d-6d68-4b3d-ccb5-5848ae4e3e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2006 Formula 1 World Championship was won by Fernando Alonso. He was driving for the Renault team.\n"
     ]
    }
   ],
   "source": [
    "#Few shots\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
    "outputId": "75f63fe3-0efc-45ed-dd45-71dbbb08d7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The winner of the 2019 Formula 1 World Championship was Lewis Hamilton.  \n",
      "Team: Mercedes.\n"
     ]
    }
   ],
   "source": [
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86",
   "metadata": {
    "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86"
   },
   "source": [
    "We've been creating the prompt without using OpenAI's roles, and as we've seen, it worked correctly.\n",
    "\n",
    "However, the proper way to do this is by using these roles to construct the prompt, making the model's learning process even more effective.\n",
    "\n",
    "By not feeding it the entire prompt as if they were system commands, we enable the model to learn from a conversation, which is more realistic for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
    "outputId": "868d2040-ca3c-4a47-a1e8-1e08d581191d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The winner of the 2019 Formula 1 World Championship was Lewis Hamilton.  \n",
      "He drove for the Mercedes-AMG Petronas Formula One Team and secured his sixth World Drivers' Championship title that year.\n"
     ]
    }
   ],
   "source": [
    "#Recomended solution\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are and expert in f1.\\n\\n'},\n",
    "    {'role':'user', 'content':'Who won the 2010 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Sebastian Vettel. \\nTeam: Red Bull. \\nPoints: 256. \"\"\"},\n",
    "    {'role':'user', 'content':'Who won the 2009 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Jenson Button. \\nTeam: BrawnGP. \\nPoints: 95. \"\"\"},\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f6b42-f351-496b-a7e8-1286426457eb",
   "metadata": {
    "id": "ac6f6b42-f351-496b-a7e8-1286426457eb"
   },
   "source": [
    "We could also address it by using a more conventional prompt, describing what we want and how we want the format.\n",
    "\n",
    "However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
    "outputId": "4c970dde-37ff-41a9-8d4e-37bb727f47a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton  \n",
      "Team: Mercedes  \n",
      "Points: 413\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\"\"\"You are and expert in f1.\n",
    "    You are going to answer the question of the user giving the name of the rider,\n",
    "    the name of the team and the points of the champion, following the format:\n",
    "    Drive:\n",
    "    Team:\n",
    "    Points: \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "KNDL1GzVngyL",
   "metadata": {
    "id": "KNDL1GzVngyL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The winner of the 2006 F1 championship was driver Fernando Alonso with the team Renault.\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are classifying .\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qZPNTLMPnkQ4",
   "metadata": {
    "id": "qZPNTLMPnkQ4"
   },
   "source": [
    "Few Shots for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ejcstgTxnnX5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejcstgTxnnX5",
    "outputId": "4b91cc73-18f6-4944-a46b-806b02b7becb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
    "\n",
    "     It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
    "     Sentiment: Positive\n",
    "\n",
    "     It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
    "     Sentiment: Negative.\n",
    "\n",
    "     I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
    "     Sentiment: Neutral\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"I'm not going to return it, but I don't plan to buy it again.\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15",
   "metadata": {
    "id": "ZHr_75sDqDJp"
   },
   "source": [
    "# Exercise\n",
    " - Complete the prompts similar to what we did in class. \n",
    "     - Try at least 3 versions\n",
    "     - Be creative\n",
    " - Write a one page report summarizing your findings.\n",
    "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
    " - What did you learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9adda59c-ad09-4e9d-88cd-54f42384a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-participant conversation with 'name' field ===\n",
      "Absolutely! I highly recommend \"Crazy Rich Asians.\" It's funny, heartwarming, and has a great cast. What about you, Alice? Any other Marvel movies besides Avengers: Endgame that you really like?\n",
      "\n",
      "=== Explanation of other message fields ===\n",
      "1. 'name': Used to identify speakers in multi-participant conversations\n",
      "2. 'tool_calls': Used when assistant wants to call external functions\n",
      "3. 'tool_call_id': Links tool responses back to their requests\n",
      "4. 'content': Can be string or array (for multimodal content in vision models)\n",
      "\n",
      "Note: Tool calling requires specific API setup and function definitions.\n",
      "Absolutely! I highly recommend \"Crazy Rich Asians.\" It's funny, heartwarming, and has a great cast. What about you, Alice? Any other Marvel movies besides Avengers: Endgame that you really like?\n",
      "\n",
      "=== Explanation of other message fields ===\n",
      "1. 'name': Used to identify speakers in multi-participant conversations\n",
      "2. 'tool_calls': Used when assistant wants to call external functions\n",
      "3. 'tool_call_id': Links tool responses back to their requests\n",
      "4. 'content': Can be string or array (for multimodal content in vision models)\n",
      "\n",
      "Note: Tool calling requires specific API setup and function definitions.\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of additional message fields\n",
    "\n",
    "# Example 1: Using 'name' field for multi-participant conversation\n",
    "context_with_names = [\n",
    "    {'role': 'system', 'content': 'You are facilitating a discussion between Alice and Bob about their favorite movies.'},\n",
    "    {'role': 'user', 'content': 'I love action movies, especially Marvel films!', 'name': 'Alice'},\n",
    "    {'role': 'user', 'content': 'I prefer romantic comedies myself.', 'name': 'Bob'},\n",
    "    {'role': 'assistant', 'content': 'Interesting! Alice enjoys action-packed Marvel movies while Bob prefers romantic comedies. What specific movies do you each recommend?'},\n",
    "    {'role': 'user', 'content': 'Definitely Avengers: Endgame!', 'name': 'Alice'},\n",
    "]\n",
    "\n",
    "print(\"=== Multi-participant conversation with 'name' field ===\")\n",
    "response = return_OAIResponse(\"What about you Bob, any specific romantic comedy recommendations?\", context_with_names)\n",
    "print(response)\n",
    "print()\n",
    "\n",
    "# Example 2: Demonstrating different content types\n",
    "context_varied_content = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant that can handle various types of content.'},\n",
    "    {'role': 'user', 'content': [\n",
    "        {'type': 'text', 'text': 'What can you tell me about this?'},\n",
    "        # Note: Image content would go here in vision models, but gpt-4-mini doesn't support images\n",
    "    ]},\n",
    "]\n",
    "\n",
    "# Example 3: Showing how tool messages would work (conceptual, not executable without function setup)\n",
    "context_with_tools_concept = [\n",
    "    {'role': 'system', 'content': 'You are a weather assistant that can check current conditions.'},\n",
    "    {'role': 'user', 'content': 'What\\'s the weather in Paris?'},\n",
    "    # This would be the assistant's response if it had tool calling enabled:\n",
    "    {\n",
    "        'role': 'assistant', \n",
    "        'content': None,\n",
    "        'tool_calls': [\n",
    "            {\n",
    "                'id': 'call_abc123',\n",
    "                'type': 'function',\n",
    "                'function': {\n",
    "                    'name': 'get_current_weather',\n",
    "                    'arguments': '{\"location\": \"Paris\", \"unit\": \"celsius\"}'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    # This would be the tool response:\n",
    "    {\n",
    "        'role': 'tool',\n",
    "        'content': '{\"location\": \"Paris\", \"temperature\": \"15\", \"condition\": \"partly cloudy\"}',\n",
    "        'tool_call_id': 'call_abc123'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=== Explanation of other message fields ===\")\n",
    "print(\"1. 'name': Used to identify speakers in multi-participant conversations\")\n",
    "print(\"2. 'tool_calls': Used when assistant wants to call external functions\")\n",
    "print(\"3. 'tool_call_id': Links tool responses back to their requests\") \n",
    "print(\"4. 'content': Can be string or array (for multimodal content in vision models)\")\n",
    "print()\n",
    "print(\"Note: Tool calling requires specific API setup and function definitions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae21f45",
   "metadata": {},
   "source": [
    "## Additional Message Fields in OpenAI Chat API\n",
    "\n",
    "Besides `role` and `content`, there are several other optional fields you can include:\n",
    "\n",
    "### Available Fields:\n",
    "1. **`name`** - Identifies the speaker (useful for multi-participant conversations)\n",
    "2. **`tool_calls`** - Array of function calls (for assistant messages)\n",
    "3. **`tool_call_id`** - Links tool responses back to requests (for tool messages)\n",
    "\n",
    "### Role-Specific Usage:\n",
    "- **System messages**: Only `role` and `content`\n",
    "- **User messages**: Can include `name` for identification\n",
    "- **Assistant messages**: Can include `tool_calls` for function calling\n",
    "- **Tool messages**: Must include `tool_call_id` to link responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19b873",
   "metadata": {},
   "source": [
    "## Function Calling - Connecting to Actual Code\n",
    "\n",
    "To connect OpenAI function calls to real code execution, you need:\n",
    "1. Define your functions in Python\n",
    "2. Create function schemas for OpenAI\n",
    "3. Handle the function calling loop\n",
    "4. Execute the actual functions and return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define your actual Python functions\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather for a given location\"\"\"\n",
    "    # This would normally call a real weather API\n",
    "    # For demo purposes, we'll return mock data\n",
    "    weather_data = {\n",
    "        \"New York\": {\"temperature\": \"72\", \"condition\": \"sunny\", \"humidity\": \"45%\"},\n",
    "        \"London\": {\"temperature\": \"59\", \"condition\": \"cloudy\", \"humidity\": \"78%\"},\n",
    "        \"Tokyo\": {\"temperature\": \"68\", \"condition\": \"rainy\", \"humidity\": \"85%\"},\n",
    "        \"Paris\": {\"temperature\": \"64\", \"condition\": \"partly cloudy\", \"humidity\": \"52%\"}\n",
    "    }\n",
    "    \n",
    "    # Simple lookup with fallback\n",
    "    weather = weather_data.get(location, {\"temperature\": \"70\", \"condition\": \"unknown\", \"humidity\": \"50%\"})\n",
    "    \n",
    "    if unit == \"celsius\":\n",
    "        # Convert from fahrenheit to celsius\n",
    "        temp_f = int(weather[\"temperature\"])\n",
    "        temp_c = round((temp_f - 32) * 5/9)\n",
    "        weather[\"temperature\"] = str(temp_c)\n",
    "        weather[\"unit\"] = \"¬∞C\"\n",
    "    else:\n",
    "        weather[\"unit\"] = \"¬∞F\"\n",
    "    \n",
    "    return f\"Weather in {location}: {weather['temperature']}{weather['unit']}, {weather['condition']}, humidity: {weather['humidity']}\"\n",
    "\n",
    "def calculate_math(expression):\n",
    "    \"\"\"Safely calculate a mathematical expression\"\"\"\n",
    "    try:\n",
    "        # Only allow safe mathematical operations\n",
    "        allowed_chars = set('0123456789+-*/.() ')\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return \"Error: Invalid characters in expression\"\n",
    "        \n",
    "        result = eval(expression)\n",
    "        return f\"The result of {expression} is {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error calculating {expression}: {str(e)}\"\n",
    "\n",
    "# Test the functions\n",
    "print(\"Testing functions:\")\n",
    "print(get_current_weather(\"New York\"))\n",
    "print(calculate_math(\"25 + 17 * 2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb23838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define function schemas for OpenAI\n",
    "# These tell the model what functions are available and how to call them\n",
    "function_definitions = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city name, e.g. San Francisco\"\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"Temperature unit\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\", \n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_math\",\n",
    "            \"description\": \"Calculate a mathematical expression\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Mathematical expression to evaluate, e.g. '2 + 2' or '15 * 3'\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create a mapping from function names to actual functions\n",
    "available_functions = {\n",
    "    \"get_current_weather\": get_current_weather,\n",
    "    \"calculate_math\": calculate_math\n",
    "}\n",
    "\n",
    "print(\"Function definitions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a function to handle the complete function calling flow\n",
    "def chat_with_functions(user_message, conversation_history=None):\n",
    "    \"\"\"Handle a conversation that can include function calls\"\"\"\n",
    "    \n",
    "    if conversation_history is None:\n",
    "        conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that can check weather and do math calculations.\"}\n",
    "        ]\n",
    "    \n",
    "    # Add user message\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    # Make initial request with function definitions\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Updated model name\n",
    "        messages=conversation_history,\n",
    "        tools=function_definitions,  # Pass available functions\n",
    "        tool_choice=\"auto\"  # Let model decide when to call functions\n",
    "    )\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    conversation_history.append(response_message)\n",
    "    \n",
    "    # Check if the model wants to call functions\n",
    "    if response_message.tool_calls:\n",
    "        print(\"üîß Model wants to call functions:\")\n",
    "        \n",
    "        # Execute each function call\n",
    "        for tool_call in response_message.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            print(f\"  Calling {function_name} with args: {function_args}\")\n",
    "            \n",
    "            # Call the actual function\n",
    "            if function_name in available_functions:\n",
    "                function_result = available_functions[function_name](**function_args)\n",
    "                print(f\"  Result: {function_result}\")\n",
    "                \n",
    "                # Add function result to conversation\n",
    "                conversation_history.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"content\": function_result\n",
    "                })\n",
    "            else:\n",
    "                print(f\"  ‚ùå Unknown function: {function_name}\")\n",
    "        \n",
    "        # Get final response from model after function calls\n",
    "        final_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=conversation_history\n",
    "        )\n",
    "        \n",
    "        final_message = final_response.choices[0].message\n",
    "        conversation_history.append(final_message)\n",
    "        \n",
    "        return final_message.content, conversation_history\n",
    "    \n",
    "    else:\n",
    "        # No function calls needed\n",
    "        return response_message.content, conversation_history\n",
    "\n",
    "# Import json for parsing function arguments\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fef991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Test the function calling system\n",
    "print(\"=== Testing Function Calling System ===\\n\")\n",
    "\n",
    "# Test 1: Weather function\n",
    "print(\"Test 1: Weather query\")\n",
    "response, history = chat_with_functions(\"What's the weather like in New York?\")\n",
    "print(f\"Final response: {response}\")\n",
    "print()\n",
    "\n",
    "# Test 2: Math function  \n",
    "print(\"Test 2: Math calculation\")\n",
    "response, history = chat_with_functions(\"What's 25 * 8 + 15?\", conversation_history=None)\n",
    "print(f\"Final response: {response}\")\n",
    "print()\n",
    "\n",
    "# Test 3: Multiple function calls in one request\n",
    "print(\"Test 3: Multiple functions\")\n",
    "response, history = chat_with_functions(\"What's the weather in London and calculate 100 / 5 for me?\")\n",
    "print(f\"Final response: {response}\")\n",
    "print()\n",
    "\n",
    "# Test 4: No function needed\n",
    "print(\"Test 4: Regular conversation\")\n",
    "response, history = chat_with_functions(\"Tell me a joke about programming\")\n",
    "print(f\"Final response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863bbb9b",
   "metadata": {},
   "source": [
    "## How Function Calling Works - Step by Step\n",
    "\n",
    "1. **Define Python Functions**: Write actual functions that do the work\n",
    "2. **Create Function Schemas**: Tell OpenAI what functions exist and their parameters\n",
    "3. **Make API Call**: Include `tools` parameter with function definitions\n",
    "4. **Check Response**: See if model wants to call functions (`tool_calls`)\n",
    "5. **Execute Functions**: Run the actual Python code\n",
    "6. **Return Results**: Send function results back to model\n",
    "7. **Get Final Response**: Model uses function results to answer user\n",
    "\n",
    "### Key Points:\n",
    "- The model decides when to call functions based on user input\n",
    "- You execute the actual Python code, not OpenAI\n",
    "- Function results are sent back to continue the conversation\n",
    "- Multiple functions can be called in a single request"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
